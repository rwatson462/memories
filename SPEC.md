# Memories — Technical Specification

> Generated by Solution Architect (Archie) | Date: 2026-02-21
> Status: DRAFT | Version: 1.0

## 1. Overview

### 1.1 Problem Statement
AI agents (Claude Code, Codex, Gemini, custom bots) lack persistent, shared memory across sessions, projects, and agent identities. **Memories** is a CLI tool that lets any agent store observations, preferences, and facts, then retrieve them semantically — enabling continuity and cross-agent intelligence.

### 1.2 Goals & Success Criteria
- An agent can store a memory and retrieve it semantically in a subsequent session.
- Memories are filterable by agent, personality, project, type, and global scope.
- Contextual memories lose confidence over time; stable memories do not.
- Reinforceable memories regain confidence when explicitly reinforced.
- Any tool that can execute shell commands can interact with the memory system.
- JSON output is parseable by AI agents without post-processing.

### 1.3 Scope

#### In Scope
- CLI tool (`memory`) for storing, searching, retrieving, reinforcing, and deleting memories
- Semantic similarity search via ChromaDB embeddings
- Decay system with three policies (stable, contextual, reinforceable)
- Predefined metadata fields for filtering (agent, personality, project, type, global)
- Soft-delete
- JSON (default) and human-readable text output formats
- Docker Compose for ChromaDB server
- Python package installable via pipx

#### Out of Scope
- HTTP/REST API (may be added later for cross-machine access)
- Authentication and access control
- Human-facing UI
- Multi-user tenancy
- Horizontal scaling / clustering
- Backup and replication
- Duplicate detection (agents are responsible for querying before storing)
- Memory versioning (agents delete and recreate instead)
- Freeform/arbitrary tags (predefined metadata fields only)
- Tag discovery commands

### 1.4 Constraints
- Python 3.11+
- ChromaDB as sole data store (no secondary database)
- Single-user, trusted-network deployment
- ChromaDB default embedding model (all-MiniLM-L6-v2) — configurable later

---

## 2. System Architecture

### 2.1 Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│  User's Machine                                             │
│                                                             │
│  ┌─────────────────────┐       ┌──────────────────────────┐ │
│  │  memory CLI          │─HTTP─▶│  ChromaDB Server         │ │
│  │  (installed via pipx)│       │  (Docker container)      │ │
│  │                     │       │  Port: 8000              │ │
│  │  • Typer commands   │       │  Volume: chroma_data     │ │
│  │  • Business logic   │       │                          │ │
│  │  • Decay computation│       └──────────────────────────┘ │
│  └─────────────────────┘                                    │
│       ▲                                                     │
│       │ shell exec                                          │
│  ┌────┴────────────────┐                                    │
│  │  AI Agents           │                                    │
│  │  (Claude, Codex, etc)│                                    │
│  └─────────────────────┘                                    │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 Component Breakdown

#### CLI Layer (`cli.py`)
- **Responsibility:** Parse commands, validate input, format output, invoke service layer.
- **Technology:** Typer (built on Click)
- **Interfaces:** Shell commands in, stdout (JSON/text) out, stderr for errors.
- **Key Design Decision:** Typer chosen for type-hint-driven argument parsing, auto-generated `--help`, and clean subcommand support. JSON is the default output format because AI agents are the primary consumers.

#### Service Layer (`memory_service.py`, `decay.py`)
- **Responsibility:** Business logic — orchestrate storage operations, compute decay/confidence, enforce soft-delete filtering.
- **Technology:** Pure Python, Pydantic models for data validation.
- **Interfaces:** Called by CLI layer; calls vector store abstraction.
- **Key Design Decision:** Confidence is computed lazily at query time (no background workers). The service layer never returns deleted memories unless explicitly requested.

#### Vector Store Abstraction (`vector_store.py`)
- **Responsibility:** Define the interface for vector storage operations.
- **Technology:** Python `Protocol` (typing module).
- **Interfaces:** `store()`, `get()`, `search()`, `delete()`, `update_metadata()`.
- **Key Design Decision:** Thin abstraction layer — only one implementation (ChromaDB) exists today, but the Protocol allows swapping the backend later without touching service or CLI code.

#### ChromaDB Adapter (`chromadb_adapter.py`)
- **Responsibility:** Implement the VectorStore Protocol using ChromaDB's Python client in HTTP/client mode.
- **Technology:** `chromadb` Python package (client mode connecting to server).
- **Interfaces:** Implements VectorStore Protocol; connects to ChromaDB server via HTTP.
- **Key Design Decision:** ChromaDB runs as a separate Docker container for persistence and potential concurrency. The adapter uses ChromaDB's HTTP client, not the embedded mode.

### 2.3 Data Flow

**Store a memory:**
```
Agent → `memory create "content" --agent claude --project foo --decay stable`
  → CLI parses args, builds MemoryCreate model
  → Service generates UUID, sets created_at, confidence=1.0, deleted=false
  → Adapter calls chromadb_collection.add(id, document, metadata)
  → CLI prints MemoryResponse as JSON to stdout
```

**Search memories:**
```
Agent → `memory search "python CLI preferences" --project foo --limit 5`
  → CLI parses args, builds SearchQuery model
  → Service builds ChromaDB where-filter: {project: "foo", deleted: false}
  → Adapter calls chromadb_collection.query(query_text, n_results, where)
  → Service computes confidence for each result (decay formula)
  → Service filters out results below min_confidence threshold
  → CLI prints SearchResponse as JSON to stdout
```

**Reinforce a memory:**
```
Agent → `memory reinforce <id>`
  → CLI passes ID to service
  → Service fetches memory, verifies decay_policy is "reinforceable"
  → Service updates last_reinforced_at to now via adapter.update_metadata()
  → CLI prints confirmation as JSON to stdout
```

---

## 3. Data Model

### 3.1 ChromaDB Document Structure

Each memory is stored as a single ChromaDB document:

| Field | ChromaDB Concept | Type | Description |
|-------|-----------------|------|-------------|
| `id` | Document ID | `str` (UUID) | Service-generated unique identifier |
| `content` | Document text | `str` | Memory content (auto-embedded by ChromaDB) |
| `agent` | Metadata | `str` | AI platform: "claude", "codex", "gemini", etc. |
| `personality` | Metadata | `str` | Agent role/persona: "engineer", "architect", "rex" |
| `project` | Metadata | `str` | Project scope (empty string if not set) |
| `type` | Metadata | `str` | Memory kind: "preference", "fact", "observation" |
| `global` | Metadata | `bool` | `true` if memory spans all contexts |
| `decay_policy` | Metadata | `str` | "stable", "contextual", "reinforceable" |
| `created_at` | Metadata | `str` | ISO 8601 timestamp |
| `last_reinforced_at` | Metadata | `str` | ISO 8601 timestamp (empty string if never reinforced) |
| `deleted` | Metadata | `bool` | Soft-delete flag, `false` by default |

**Notes:**
- `confidence` is NOT stored — it is computed at query time from `decay_policy`, `created_at`, and `last_reinforced_at`.
- ChromaDB metadata values are limited to `str`, `int`, `float`, `bool`. All fields conform to this.
- Empty/unset optional fields use empty string `""` (not null) because ChromaDB metadata does not support null values.
- All five filterable fields (`agent`, `personality`, `project`, `type`, `global`) are always written on every document, using empty string as the "not set" sentinel for string fields and `false` as default for `global`.

### 3.2 Storage Strategy

- **Single ChromaDB collection** named `memories` (configurable via env var).
- ChromaDB server manages persistence via Docker volume (`chroma_data`).
- No migrations — ChromaDB is schemaless. Metadata fields can be added in future versions without migration.
- Embedding model: ChromaDB default (`all-MiniLM-L6-v2`, 384 dimensions).

### 3.3 Access Patterns

| Operation | ChromaDB Method | Filters |
|-----------|----------------|---------|
| Store memory | `collection.add()` | — |
| Semantic search | `collection.query()` | `where={deleted: false}` + optional tag filters |
| Get by ID | `collection.get(ids=[id])` | Check `deleted` flag in application code |
| Soft-delete | `collection.update(metadata={deleted: true})` | — |
| Reinforce | `collection.update(metadata={last_reinforced_at: now})` | — |
| Health check | `chromadb_client.heartbeat()` | — |

---

## 4. CLI Interface

### 4.1 Command Overview

The CLI is registered as `memory` via the `[project.scripts]` entry point in `pyproject.toml`.

### 4.2 Command Specifications

#### `memory create <content>`
- **Purpose:** Store a new memory in ChromaDB.
- **Arguments:**
  - `content` (str, required, positional): The memory text.
- **Options:**
  - `--agent` (str, optional): AI platform identifier.
  - `--personality` (str, optional): Agent role/persona.
  - `--project` (str, optional): Project scope.
  - `--type` (str, optional): Memory kind (preference, fact, observation).
  - `--global` (bool, flag): Mark as global memory.
  - `--decay` (str, optional, default "stable"): Decay policy — stable, contextual, or reinforceable.
  - `--format` (str, optional, default "json"): Output format — json or text.
- **Output (JSON):**
  ```json
  {
    "id": "a1b2c3d4-...",
    "content": "User prefers Python for CLI tools",
    "agent": "claude",
    "personality": "engineer",
    "project": "memories",
    "type": "preference",
    "global": false,
    "decay_policy": "stable",
    "confidence": 1.0,
    "created_at": "2026-02-21T10:00:00Z"
  }
  ```
- **Exit codes:** 0 on success, 1 on error.

#### `memory search <query>`
- **Purpose:** Semantic similarity search across memories.
- **Arguments:**
  - `query` (str, required, positional): Search query text.
- **Options:**
  - `--agent` (str, optional): Filter by agent.
  - `--personality` (str, optional): Filter by personality.
  - `--project` (str, optional): Filter by project.
  - `--type` (str, optional): Filter by type.
  - `--global` (bool, flag): Filter for global memories only.
  - `--limit` (int, optional, default 10): Max results to return.
  - `--min-confidence` (float, optional, default 0.3): Minimum confidence threshold.
  - `--format` (str, optional, default "json"): Output format.
- **Output (JSON):**
  ```json
  {
    "results": [
      {
        "id": "a1b2c3d4-...",
        "content": "User prefers Python for CLI tools",
        "agent": "claude",
        "personality": "engineer",
        "project": "memories",
        "type": "preference",
        "global": false,
        "decay_policy": "stable",
        "confidence": 0.95,
        "similarity": 0.87,
        "created_at": "2026-02-21T10:00:00Z"
      }
    ],
    "count": 1
  }
  ```
- **Notes:** Results are ordered by similarity (ChromaDB default). Confidence is computed at query time. Results below `min-confidence` are excluded. The `deleted: false` filter is always applied.

#### `memory get <id>`
- **Purpose:** Retrieve a specific memory by ID.
- **Arguments:**
  - `id` (str, required, positional): Memory UUID.
- **Options:**
  - `--format` (str, optional, default "json"): Output format.
- **Output (JSON):** Same shape as a single search result (without `similarity` field).
- **Errors:** Exit code 1 with `{"error": "Memory not found"}` if ID doesn't exist or is deleted.

#### `memory reinforce <id>`
- **Purpose:** Reset the decay timer for a reinforceable memory.
- **Arguments:**
  - `id` (str, required, positional): Memory UUID.
- **Options:**
  - `--format` (str, optional, default "json"): Output format.
- **Output (JSON):**
  ```json
  {
    "id": "a1b2c3d4-...",
    "confidence": 1.0,
    "last_reinforced_at": "2026-02-21T12:00:00Z"
  }
  ```
- **Errors:**
  - Exit code 1 if memory not found or deleted.
  - Exit code 1 with `{"error": "Memory has stable decay policy, reinforcement has no effect"}` if decay_policy is "stable".
  - Exit code 1 with `{"error": "Memory has contextual decay policy, reinforcement is not supported"}` if decay_policy is "contextual".

#### `memory delete <id>`
- **Purpose:** Soft-delete a memory (sets `deleted: true` in metadata).
- **Arguments:**
  - `id` (str, required, positional): Memory UUID.
- **Options:**
  - `--format` (str, optional, default "json"): Output format.
- **Output (JSON):**
  ```json
  {
    "id": "a1b2c3d4-...",
    "deleted": true
  }
  ```
- **Errors:** Exit code 1 if memory not found or already deleted.

#### `memory status`
- **Purpose:** Check ChromaDB connection health and report status.
- **Options:**
  - `--format` (str, optional, default "json"): Output format.
- **Output (JSON):**
  ```json
  {
    "status": "healthy",
    "chromadb_host": "localhost:8000",
    "collection": "memories",
    "memory_count": 42
  }
  ```
- **Errors:** Exit code 1 with `{"status": "unhealthy", "error": "Cannot connect to ChromaDB at localhost:8000"}` if connection fails.

### 4.3 Output Formatting

- **JSON (default):** All output is valid JSON printed to stdout. Errors are JSON objects with an `"error"` key printed to stderr.
- **Text (`--format text`):** Human-readable formatted output. Intended for debugging and manual inspection. Implementation can be minimal for v1 — pretty-printed key-value pairs are sufficient.
- **Exit codes:** 0 = success, 1 = error (all commands).

---

## 5. Decay System

### 5.1 Decay Policies

| Policy | Behavior | Use Case |
|--------|----------|----------|
| `stable` | Confidence is always 1.0. Never decays. | Facts, preferences, decisions that persist until explicitly contradicted. |
| `contextual` | Confidence decays linearly from 1.0 to 0.0 over `DECAY_HALF_LIFE_HOURS`. | Observations and events tied to a moment. |
| `reinforceable` | Same linear decay as contextual, but age is measured from `last_reinforced_at` (or `created_at` if never reinforced). Reinforcement resets the clock. | Patterns and conventions that stay relevant as long as they're in active use. |

### 5.2 Confidence Computation

Computed at query time. Never stored.

```python
def compute_confidence(decay_policy: str, created_at: datetime, last_reinforced_at: datetime | None, half_life_hours: float) -> float:
    if decay_policy == "stable":
        return 1.0

    reference_time = last_reinforced_at if (decay_policy == "reinforceable" and last_reinforced_at) else created_at
    age_hours = (now() - reference_time).total_seconds() / 3600

    confidence = max(0.0, 1.0 - (age_hours / half_life_hours))
    return round(confidence, 4)
```

### 5.3 Confidence Filtering

- Search results with confidence below `min_confidence` (default 0.3, configurable) are excluded.
- `memory get` returns the memory regardless of confidence (but includes the computed confidence value).
- Memories with confidence 0.0 are effectively invisible in search but not deleted.

---

## 6. Infrastructure & Deployment

### 6.1 Docker Compose

ChromaDB runs as a Docker container. The CLI runs on the host machine.

```yaml
# docker-compose.yml
services:
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "${CHROMADB_PORT:-8000}:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - ANONYMIZED_TELEMETRY=false

volumes:
  chroma_data:
```

### 6.2 Python Package

```toml
# pyproject.toml (key sections)
[project]
name = "memories"
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "typer>=0.9",
    "chromadb>=0.4",
    "pydantic>=2.0",
    "pydantic-settings>=2.0",
]

[project.scripts]
memory = "memories.cli:app"
```

### 6.3 Installation

```bash
# Start ChromaDB
docker compose up -d

# Install CLI
pipx install -e .

# Verify
memory status
```

---

## 7. Configuration

All configuration via environment variables, with sensible defaults for local development.

| Variable | Default | Description |
|----------|---------|-------------|
| `CHROMADB_HOST` | `localhost` | ChromaDB server hostname |
| `CHROMADB_PORT` | `8000` | ChromaDB server port |
| `COLLECTION_NAME` | `memories` | ChromaDB collection name |
| `DEFAULT_LIMIT` | `10` | Default search result count |
| `MIN_CONFIDENCE` | `0.3` | Default minimum confidence threshold |
| `DECAY_HALF_LIFE_HOURS` | `720` | Hours until contextual/reinforceable memories reach confidence 0.0 (default: 30 days) |

Configuration is loaded via `pydantic-settings` which reads from environment variables and `.env` files.

---

## 8. Cross-Cutting Concerns

### 8.1 Error Handling

- All errors are returned as JSON to stderr: `{"error": "Human-readable message"}`.
- CLI exits with code 1 on any error.
- ChromaDB connection errors are caught and reported with actionable messages (e.g., "Cannot connect to ChromaDB at localhost:8000. Is Docker running?").
- Invalid arguments are caught by Typer's built-in validation before reaching service code.

### 8.2 Security

- Trusted network model — no authentication, no authorization.
- ChromaDB binds to localhost only (Docker port mapping: `127.0.0.1:8000:8000` in production).
- No secrets to manage in v1.
- Content is stored as-is — no sanitization needed since there is no UI rendering context.

### 8.3 Performance

- Embedding is handled by ChromaDB server-side (all-MiniLM-L6-v2 is fast, ~50ms per embedding).
- CLI is a thin client — startup time is dominated by Python import overhead, not business logic.
- No caching layer — ChromaDB handles its own caching internally.
- At single-user scale with moderate memory counts (hundreds to low thousands), no performance concerns are expected.

---

## 9. Testing Strategy

### 9.1 Test Levels

| Level | Scope | Tool | Approach |
|-------|-------|------|----------|
| **Unit** | Decay computation, model validation | pytest | Pure functions, no external dependencies. Mock datetime for deterministic decay tests. |
| **Unit** | Memory service logic | pytest | Mock the VectorStore Protocol. Test business logic (confidence filtering, soft-delete enforcement, reinforce validation). |
| **Integration** | ChromaDB adapter | pytest | Run against a real ChromaDB instance (Docker). Test store/get/search/delete/update_metadata round-trips. |
| **CLI** | End-to-end commands | pytest + `typer.testing.CliRunner` | Invoke CLI commands programmatically. Assert JSON output structure and exit codes. Requires running ChromaDB. |

### 9.2 Coverage Goals

- 100% coverage on decay computation (it's pure math with only 3 branches).
- All CLI commands tested for success and primary error paths.
- Integration tests verify ChromaDB metadata filtering works as expected for all filterable fields.

---

## 10. Project Structure

```
memories/
├── docker-compose.yml
├── pyproject.toml
├── .env.example
├── README.md                          # Setup and usage instructions
├── src/
│   └── memories/
│       ├── __init__.py
│       ├── cli.py                     # Typer app — command definitions
│       ├── config.py                  # Settings (pydantic-settings)
│       ├── models.py                  # Pydantic models + enums
│       ├── services/
│       │   ├── __init__.py
│       │   ├── memory_service.py      # Business logic orchestration
│       │   └── decay.py               # Confidence computation
│       └── stores/
│           ├── __init__.py
│           ├── vector_store.py        # VectorStore Protocol
│           └── chromadb_adapter.py    # ChromaDB implementation
└── tests/
    ├── __init__.py
    ├── conftest.py                    # Fixtures (test ChromaDB collection)
    ├── test_cli.py                    # CLI command tests
    ├── test_memory_service.py         # Service unit tests
    ├── test_decay.py                  # Decay computation tests
    └── test_chromadb_adapter.py       # Adapter integration tests
```

---

## 11. Implementation Plan

### 11.1 Milestones

| # | Milestone | Deliverable | Depends On |
|---|-----------|-------------|------------|
| M1 | Project Scaffolding | Installable package, ChromaDB running | — |
| M2 | Core Infrastructure | Config, models, vector store protocol + adapter | M1 |
| M3 | Service Layer | Business logic for all memory operations | M2 |
| M4 | CLI Interface | All commands wired and producing output | M3 |
| M5 | Testing | Unit + integration test suite | M4 |
| M6 | Polish | Error messages, text output format, .env.example | M5 |

### 11.2 Task Breakdown

#### Milestone 1: Project Scaffolding
- [ ] Create `pyproject.toml` with project metadata, dependencies (typer, chromadb, pydantic, pydantic-settings, pytest), and `[project.scripts]` entry point — `pyproject.toml` — S
- [ ] Create directory structure: `src/memories/`, `src/memories/services/`, `src/memories/stores/`, `tests/` with `__init__.py` files — multiple files — S
- [ ] Create `docker-compose.yml` with ChromaDB service, volume, and port mapping — `docker-compose.yml` — S
- [ ] Create `.env.example` with all configuration variables and defaults — `.env.example` — S
- [ ] Verify: `docker compose up -d` starts ChromaDB; `pip install -e .` registers the `memory` command — manual verification — S

#### Milestone 2: Core Infrastructure
- [ ] Implement `config.py` — Settings class using pydantic-settings, reads env vars with defaults — `src/memories/config.py` — S
- [ ] Implement `models.py` — Pydantic models: `MemoryCreate`, `MemoryResponse`, `SearchQuery`, `SearchResponse`, `SearchResultItem`; Enums: `DecayPolicy`, `OutputFormat` — `src/memories/models.py` — M
- [ ] Implement `vector_store.py` — `VectorStore` Protocol with methods: `store(id, content, metadata)`, `get(id)`, `search(query, n_results, where)`, `delete(id)`, `update_metadata(id, metadata)`, `count()`, `heartbeat()` — `src/memories/stores/vector_store.py` — S
- [ ] Implement `chromadb_adapter.py` — `ChromaDBAdapter` class implementing the VectorStore Protocol, connecting to ChromaDB via HTTP client — `src/memories/stores/chromadb_adapter.py` — M

#### Milestone 3: Service Layer
- [ ] Implement `decay.py` — `compute_confidence(decay_policy, created_at, last_reinforced_at, half_life_hours) -> float` with stable/contextual/reinforceable branches — `src/memories/services/decay.py` — S
- [ ] Implement `memory_service.py` — `MemoryService` class with methods: `create_memory()`, `search_memories()`, `get_memory()`, `reinforce_memory()`, `delete_memory()`, `get_status()`. Depends on VectorStore Protocol and decay module. — `src/memories/services/memory_service.py` — L

#### Milestone 4: CLI Interface
- [ ] Implement `cli.py` — Typer app with all 6 commands (`create`, `search`, `get`, `reinforce`, `delete`, `status`), wired to `MemoryService`. Handles JSON/text output formatting and error output to stderr. — `src/memories/cli.py` — L
- [ ] Implement `__init__.py` — Package-level setup: instantiate config, adapter, service, and expose the Typer app — `src/memories/__init__.py` — S

#### Milestone 5: Testing
- [ ] Implement `conftest.py` — Fixtures: ChromaDB test client (connects to running Docker instance), test collection (created/cleaned per test), `MemoryService` instance with real adapter — `tests/conftest.py` — S
- [ ] Implement `test_decay.py` — Unit tests for all three decay policies, boundary conditions (exactly at half-life, beyond half-life, just created), datetime mocking — `tests/test_decay.py` — S
- [ ] Implement `test_memory_service.py` — Unit tests with mocked VectorStore: create, search (confidence filtering, soft-delete filtering), get (found/not found/deleted), reinforce (valid/invalid policy), delete — `tests/test_memory_service.py` — M
- [ ] Implement `test_chromadb_adapter.py` — Integration tests against real ChromaDB: store/get round-trip, search with metadata filters, update_metadata, delete — `tests/test_chromadb_adapter.py` — M
- [ ] Implement `test_cli.py` — End-to-end CLI tests using `typer.testing.CliRunner`: all 6 commands, JSON output validation, exit codes, error cases — `tests/test_cli.py` — M

#### Milestone 6: Polish
- [ ] Implement text output formatter — human-readable output for `--format text` across all commands — `src/memories/cli.py` — S
- [ ] Review and improve error messages — actionable messages for ChromaDB connection failures, invalid IDs, invalid decay policies — `src/memories/cli.py`, `src/memories/services/memory_service.py` — S
- [ ] Lock ChromaDB Docker port to localhost — update docker-compose.yml port mapping to `127.0.0.1:8000:8000` — `docker-compose.yml` — S

### 11.3 Recommended Implementation Order

Build bottom-up: infrastructure → logic → interface → tests → polish.

1. **M1 first** — get the project installable and ChromaDB running. Everything else depends on this.
2. **M2 next** — config, models, and the adapter are the foundation. Test the adapter manually against ChromaDB to validate the integration before building service logic on top.
3. **M3 then M4** — service layer before CLI, so each CLI command is a thin wrapper over tested service methods.
4. **M5 after M4** — write tests once the full stack is wired. Integration tests validate the real flow; unit tests lock down edge cases.
5. **M6 last** — polish is non-critical and shouldn't block usability.

### 11.4 Risk Register

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| ChromaDB metadata filtering doesn't support the `where` clause combinations we need (e.g., combining `deleted: false` with tag filters) | Low | High | ChromaDB supports `$and` operator for combining filters. Verify in M2 during adapter implementation. |
| CLI startup time is slow due to Python + ChromaDB client import overhead | Medium | Low | Acceptable for v1. If problematic, can lazy-import chromadb only when needed. |
| Concurrent CLI invocations from multiple agents cause race conditions (e.g., two agents reinforcing the same memory simultaneously) | Low | Low | ChromaDB server handles concurrent requests. Individual operations are atomic at the ChromaDB level. No multi-step transactions in our design. |
| ChromaDB default embedding model (all-MiniLM-L6-v2) produces poor semantic matches for agent memory content | Medium | Medium | Acceptable for v1. Model is configurable in ChromaDB. Can swap to a better model later if retrieval quality is poor. |

---

## 12. Decision Log

| ID | Decision | Rationale | Date |
|----|----------|-----------|------|
| D1 | No duplicate detection | Agents are responsible for querying before storing. Simplifies the store path and removes the need for similarity thresholds and force flags. | 2026-02-21 |
| D2 | Lazy decay at query time | Avoids background workers and keeps the system stateless between invocations. Confidence is a pure function of timestamps and policy. | 2026-02-21 |
| D3 | Filter low-confidence by default | Configurable threshold (default 0.3). Prevents stale contextual memories from cluttering search results while allowing callers to adjust. | 2026-02-21 |
| D4 | Top-K search, no pagination | Semantic similarity results degrade quickly in relevance. Returning top-K (default 10) covers the useful range without pagination complexity. | 2026-02-21 |
| D5 | Soft-delete | Agents can clean up mistakes. `deleted` metadata flag filtered on every search query. No hard delete in v1. | 2026-02-21 |
| D6 | Thin abstraction layer (Python Protocol) | One implementation (ChromaDB) today, but the Protocol allows swapping backends without touching service or CLI code. Minimal overhead for meaningful future flexibility. | 2026-02-21 |
| D7 | ChromaDB as separate Docker container | Client/server mode provides independent persistence, proper concurrency handling, and separation of concerns. CLI connects via HTTP client. | 2026-02-21 |
| D8 | No versioning | Agents delete and recreate memories instead of versioning. Eliminates version tracking, history endpoint, and the ChromaDB limitations around relational queries. | 2026-02-21 |
| D9 | ChromaDB as sole data store | Acceptable tradeoffs at single-user scale. Avoids the complexity of syncing two stores. Tag discovery and complex queries can be revisited if scale demands it. | 2026-02-21 |
| D10 | Predefined metadata fields (not freeform tags) | Fixed set: agent, personality, project, type, global. Removes need for tag discovery commands and avoids uncontrolled metadata sprawl. Can be expanded with new fields as needs emerge. | 2026-02-21 |
| D11 | CLI interface (not HTTP API) | Primary consumers are AI agents that execute shell commands (Claude Code, etc.). CLI is simpler to build, deploy, and invoke than an HTTP service. HTTP API noted as future enhancement. | 2026-02-21 |
| D12 | Typer framework | Modern, type-hint-driven argument parsing with auto-generated help. Built on Click for robustness. Minimal boilerplate. | 2026-02-21 |
| D13 | pipx distribution | Isolated install prevents dependency conflicts with user's Python environment. Standard Python packaging via pyproject.toml. | 2026-02-21 |
| D14 | JSON default output | Primary consumers are AI agents parsing stdout. JSON by default eliminates the need for agents to remember `--format json` on every invocation. `--format text` available for human debugging. | 2026-02-21 |

---

## Appendix

### A. Glossary

| Term | Definition |
|------|-----------|
| **Memory** | A stored piece of knowledge — an observation, preference, fact, or context — with metadata and a decay policy. |
| **Decay policy** | Rules governing how a memory's confidence degrades over time (stable, contextual, reinforceable). |
| **Confidence** | A 0.0–1.0 score representing how "current" a memory is. Computed at query time based on decay policy and age. |
| **Reinforcement** | Explicitly resetting the decay timer on a reinforceable memory, restoring its confidence to 1.0. |
| **Soft-delete** | Marking a memory as deleted without removing it from storage. Soft-deleted memories are excluded from search. |
| **Agent** | The AI platform (Claude, Codex, Gemini) — the software making the CLI call. |
| **Personality** | The role or persona an agent is operating as (engineer, architect, requirements-analyst) — determined by system prompt or task context. |

### B. References

- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Typer Documentation](https://typer.tiangolo.com/)
- [Pydantic Settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/)
- [ChromaDB Docker Image](https://hub.docker.com/r/chromadb/chroma)
